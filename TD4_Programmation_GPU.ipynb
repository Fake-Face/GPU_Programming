{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9qBsYf5bhHA",
        "outputId": "0f88f0c1-b009-4d39-8eb9-5222a80bc34e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFO9waXJhtaf",
        "outputId": "c50d1232-9d1c-4a68-da17-48c9787cf8d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.1.0-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "id": "m5FiXiIJh8v4",
        "outputId": "2a84899a-fe81-4100-b721-24a9d56eacb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source files will be saved in \"/tmp/tmpfczxbtli\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start please order the memory access from fastest to slowest.\\\n",
        "Registers > Shared Memory (L1 Cache) > Global Memory > Constant Memory > Texture Memory > Disk Storage (SSD/HDD)"
      ],
      "metadata": {
        "id": "y_YLk_qVuudu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxZaAXsuHaE"
      },
      "outputs": [],
      "source": [
        "__global__ void copy_vector(float *data)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  __shared__ float temp_data[256];\n",
        "  temp_data[index] = data[index];\n",
        "  __syncthreads();\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using shared memory for operations is generally more efficient than using global memory directly due to faster access speeds, reduced bandwidth demand, and enhanced thread cooperation. However, the efficiency gain depends on the operation's nature and must consider shared memory's limitations, such as its limited size and the need for thread synchronization. Overall, the method is beneficial for operations that require frequent data access by threads within the same block, making the initial data transfer to shared memory worthwhile."
      ],
      "metadata": {
        "id": "mXBDHD1uzYpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 256\n",
        "\n",
        "__global__ void copy_vector_global(float *data, int iterations) {\n",
        "    int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (index < N) {\n",
        "        int aux = 0;\n",
        "        for (int i = 0; i < iterations; ++i) {\n",
        "            aux += data[index];\n",
        "        }\n",
        "        data[index] = aux;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void copy_vector_shared(float *data, int iterations) {\n",
        "    __shared__ float temp_data[N];\n",
        "    int index = threadIdx.x;\n",
        "\n",
        "    temp_data[index] = data[index];\n",
        "    __syncthreads();\n",
        "\n",
        "    int aux = 0;\n",
        "    for (int i = 0; i < iterations; ++i) {\n",
        "        aux += temp_data[index];\n",
        "    }\n",
        "\n",
        "    data[index] = aux;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float *d_data;\n",
        "    cudaMalloc((void**)&d_data, N * sizeof(float));\n",
        "\n",
        "    // Initialize data on host and copy to device\n",
        "    float *h_data = (float*)malloc(N * sizeof(float));\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_data[i] = i;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Timing variables\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    float milliseconds = 0;\n",
        "\n",
        "    // Run and time kernel using global memory for 1 iteration\n",
        "    cudaEventRecord(start);\n",
        "    copy_vector_global<<<(N + 255) / 256, 256>>>(d_data, 1);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Global memory - 1 iteration: %f ms\\n\", milliseconds);\n",
        "\n",
        "    // Run and time kernel using shared memory for 1 iteration\n",
        "    cudaEventRecord(start);\n",
        "    copy_vector_shared<<<1, 256>>>(d_data, 1);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Shared memory - 1 iteration: %f ms\\n\", milliseconds);\n",
        "\n",
        "    // Run and time kernel using global memory for 30 iterations\n",
        "    cudaEventRecord(start);\n",
        "    copy_vector_global<<<(N + 255) / 256, 256>>>(d_data, 30);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Global memory - 30 iterations: %f ms\\n\", milliseconds);\n",
        "\n",
        "    // Run and time kernel using shared memory for 30 iterations\n",
        "    cudaEventRecord(start);\n",
        "    copy_vector_shared<<<1, 256>>>(d_data, 30);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Shared memory - 30 iterations: %f ms\\n\", milliseconds);\n",
        "\n",
        "    // Run and time kernel using global memory for 100 iterations\n",
        "    cudaEventRecord(start);\n",
        "    copy_vector_global<<<(N + 255) / 256, 256>>>(d_data, 100);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Global memory - 100 iterations: %f ms\\n\", milliseconds);\n",
        "\n",
        "    // Run and time kernel using shared memory for 100 iterations\n",
        "    cudaEventRecord(start);\n",
        "    copy_vector_shared<<<1, 256>>>(d_data, 100);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Shared memory - 100 iterations: %f ms\\n\", milliseconds);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_data);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_data);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "9Ii7Mun11159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f929c11-d139-4d94-d053-b5648878446f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global memory - 1 iteration: 53.525505 ms\n",
            "Shared memory - 1 iteration: 0.020384 ms\n",
            "Global memory - 30 iterations: 0.008192 ms\n",
            "Shared memory - 30 iterations: 0.007808 ms\n",
            "Global memory - 100 iterations: 0.012576 ms\n",
            "Shared memory - 100 iterations: 0.011296 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// CUDA kernel to add elements of two arrays\n",
        "__global__ void vecAdd(int stride, int N, int* a, int* b, int* c) {\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if (i < N) {\n",
        "    c[i] = a[(i*stride)%N] + b[(i*stride)%N];\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  cudaDeviceReset();\n",
        "  int *a, *b, *c; // Host copies of a, b, c\n",
        "  int *d_a, *d_b, *d_c; // Device copies of a, b, c\n",
        "  int n = 40960000;\n",
        "  int stride = 32;//Change this value HERE\n",
        "  int size = n * sizeof(int); // Size of the arrays in bytes\n",
        "\n",
        "  // Allocate space for host copies and setup values\n",
        "  a = (int *)malloc(size);\n",
        "  b = (int *)malloc(size);\n",
        "  c = (int *)malloc(size);\n",
        "\n",
        "  // Initialize arrays with data\n",
        "  for (int i = 0; i < n; i++) {\n",
        "    a[i] = i;\n",
        "    b[i] = i;\n",
        "  }\n",
        "\n",
        "  // Allocate space for device copies\n",
        "  cudaMalloc((void **)&d_a, size);\n",
        "  cudaMalloc((void **)&d_b, size);\n",
        "  cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "  // Copy inputs to device\n",
        "  cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Setup timing events\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // Record event on the default stream\n",
        "  cudaEventRecord(start, 0);\n",
        "\n",
        "  // Launch vecAdd() kernel on GPU with n/1024 blocks of 1024 threads each\n",
        "  vecAdd<<<(n + 1023)/1024, 1024>>>(stride, n, d_a, d_b, d_c);\n",
        "  cudaEventRecord(stop, 0);\n",
        "  cudaEventSynchronize(stop);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Copy result back to host\n",
        "  cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Record event on the default stream\n",
        "\n",
        "  // Calculate elapsed time\n",
        "  float milliseconds = 0;\n",
        "  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "  printf(\"Time for vector addition: %f ms\\n\", milliseconds);\n",
        "\n",
        "  // Cleanup\n",
        "  free(a);\n",
        "  free(b);\n",
        "  free(c);\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "kFOyA-D-aYZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0123f9b-3dee-40b6-fae5-0bef09b3582a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for vector addition: 130.853241 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void incrementVector(int N, int stride, int* data)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (index < N) {\n",
        "  index = index % stride;\n",
        "  data[index] = data[index] +1;\n",
        "  // atomicAdd(&data[index], 1); // Use atomicAdd to safely increment the value\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  const int N = 1000000;\n",
        "  const int stride = 10;\n",
        "  int data[N];\n",
        "\n",
        "  // Initialize data array\n",
        "  for (int i = 0; i < N; ++i) {\n",
        "    data[i] = 0;\n",
        "  }\n",
        "\n",
        "  int *d_data, *d_result;\n",
        "  cudaMalloc((void**)&d_data, N * sizeof(int));\n",
        "  cudaMemcpy(d_data, data, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Launch kernel\n",
        "  incrementVector<<<1000, 1000>>>(N, stride, d_data);\n",
        "\n",
        "  cudaMemcpy(data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Print the results\n",
        "  for (int i = 0; i < stride; ++i) {\n",
        "    printf(\"Result[%d] = %d\\n\", i, data[i]);\n",
        "  }\n",
        "\n",
        "  cudaFree(d_data);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8MdZIiGnGVs",
        "outputId": "b2449ff5-f870-498d-90cc-b6cd595c521e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result[0] = 53\n",
            "Result[1] = 53\n",
            "Result[2] = 53\n",
            "Result[3] = 53\n",
            "Result[4] = 53\n",
            "Result[5] = 53\n",
            "Result[6] = 53\n",
            "Result[7] = 53\n",
            "Result[8] = 53\n",
            "Result[9] = 53\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code to a file\n",
        "\n",
        "%%writefile histogram.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <map>\n",
        "\n",
        "void calculateHistogram(const std::vector<int>& data, std::map<int, int>& histogram) {\n",
        "  // Clear any previous contents in the histogram\n",
        "  histogram.clear();\n",
        "\n",
        "  // Iterate over the data and count the occurrences of each value\n",
        "  for (int value : data) {\n",
        "    histogram[value]++;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Example data\n",
        "  std::vector<int> data = {1, 2, 2, 3, 3, 3, 4, 4, 4, 4};\n",
        "  std::map<int, int> histogram;\n",
        "\n",
        "  // Calculate histogram\n",
        "  calculateHistogram(data, histogram);\n",
        "\n",
        "  // Print histogram\n",
        "  for (const auto& pair : histogram) {\n",
        "    std::cout << \"Value \" << pair.first << \" occurred \" <<\n",
        "    pair.second << \" times.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ITJCWVoItq",
        "outputId": "df067752-6748-412b-ae78-d9d90ae1d9d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting histogram.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the C code\n",
        "!g++ histogram.cpp -o histogram"
      ],
      "metadata": {
        "id": "BBznANDIoKPC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the compiled program\n",
        "!./histogram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOLieTw_oKtF",
        "outputId": "21eaa855-7efb-42e5-c7ac-687248d6e603"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value 1 occurred 1 times.\n",
            "Value 2 occurred 2 times.\n",
            "Value 3 occurred 3 times.\n",
            "Value 4 occurred 4 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void naiveHistogram(int* histogram, const int* data, int dataSize, int binSize) {\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (index < dataSize) {\n",
        "        int bin = data[index] / binSize; // Compute the bin index\n",
        "        atomicAdd(&histogram[bin], 1);   // Safely increment the bin's count\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int dataSize = 1024; // Example size\n",
        "  const int maxValue = 20; // Maximum value in the data\n",
        "  const int binSize = 2; // Size of each bin (grouping of numbers)\n",
        "\n",
        "  const int numBins = (maxValue + binSize - 1) / binSize; // Number of bins\n",
        "\n",
        "  int* d_data;\n",
        "  int* d_histogram;\n",
        "  int h_data[dataSize];\n",
        "\n",
        "  // Generate random data (for demonstration purposes)\n",
        "  for (int i = 0; i < dataSize; i++) {\n",
        "    h_data[i] = rand() % (maxValue + 1); // Random value between 0 and maxValue\n",
        "  }\n",
        "\n",
        "  // Allocate memory on the device\n",
        "  cudaMalloc(&d_data, dataSize * sizeof(int));\n",
        "  cudaMalloc(&d_histogram, numBins * sizeof(int));\n",
        "\n",
        "  // Initialize histogram to zero\n",
        "  int histogram[numBins] = {0};\n",
        "\n",
        "  // Copy data and initialized histogram to the device\n",
        "  cudaMemcpy(d_data, h_data, dataSize * sizeof(int), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_histogram, histogram, numBins * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Launch the kernel\n",
        "  naiveHistogram<<<(dataSize / 256) + 1, 256>>>(d_histogram, d_data, dataSize, binSize);\n",
        "\n",
        "  // Copy the results back to the host\n",
        "  cudaMemcpy(histogram, d_histogram, numBins * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Cleanup\n",
        "  cudaFree(d_data);\n",
        "  cudaFree(d_histogram);\n",
        "\n",
        "  // Print histogram\n",
        "  for (int i = 0; i < numBins; ++i) {\n",
        "    std::cout << \"Range \" << i*binSize << \"-\" << (i+1)*binSize - 1 << \" occurred \" << histogram[i] << \" times.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHpp3ew0om8f",
        "outputId": "037b3f00-6330-4c88-f42f-33cfc6e9a5e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range 0-1 occurred 87 times.\n",
            "Range 2-3 occurred 113 times.\n",
            "Range 4-5 occurred 96 times.\n",
            "Range 6-7 occurred 78 times.\n",
            "Range 8-9 occurred 98 times.\n",
            "Range 10-11 occurred 97 times.\n",
            "Range 12-13 occurred 110 times.\n",
            "Range 14-15 occurred 101 times.\n",
            "Range 16-17 occurred 104 times.\n",
            "Range 18-19 occurred 100 times.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "int main() {\n",
        "\n",
        "  // Get the current device ID\n",
        "  int device_id;\n",
        "  cudaGetDevice(&device_id);\n",
        "\n",
        "  // Get the properties of the current device\n",
        "  cudaDeviceProp device_prop;\n",
        "  cudaGetDeviceProperties(&device_prop, device_id);\n",
        "\n",
        "  // Output some of the device properties\n",
        "  std::cout << \"Device ID: \" << device_id << std::endl;\n",
        "  std::cout << \"Device Name: \" << device_prop.name << std::endl;\n",
        "  std::cout << \"Compute Capability: \" << device_prop.major << \".\" << device_prop.minor << std::endl;\n",
        "  std::cout << \"Total Global Memory: \" << device_prop.totalGlobalMem << \" bytes\" << std::endl;\n",
        "  std::cout << \"Max Threads per Block: \" << device_prop.maxThreadsPerBlock << std::endl;\n",
        "  std::cout << \"Max Block Dimensions: (\" << device_prop.maxThreadsDim[0] << \", \" << device_prop.maxThreadsDim[1] << \", \" << device_prop.maxThreadsDim[2] << \")\" << std::endl;\n",
        "  std::cout << \"Max Grid Dimensions: (\" << device_prop.maxGridSize[0] << \", \" << device_prop.maxGridSize[1] << \", \" << device_prop.maxGridSize[2] << \")\" << std::endl;\n",
        "\n",
        "  // Check for any errors\n",
        "  cudaError_t error = cudaGetLastError();\n",
        "  if (error != cudaSuccess) {\n",
        "    std::cerr << \"CUDA Error: \" << cudaGetErrorString(error) <<\n",
        "    std::endl;\n",
        "    return -1;\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJx1kGCNxNGg",
        "outputId": "a9aac4fa-42ce-42db-f497-8e1f849ee15b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device ID: 0\n",
            "Device Name: Tesla T4\n",
            "Compute Capability: 7.5\n",
            "Total Global Memory: 15835660288 bytes\n",
            "Max Threads per Block: 1024\n",
            "Max Block Dimensions: (1024, 1024, 64)\n",
            "Max Grid Dimensions: (2147483647, 65535, 65535)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions for this TD:\n",
        "1) What are the properties of your GPU?\n",
        "   - Device Name: Tesla T4\n",
        "   - Compute Capability: 7.5\n",
        "   - Total Global Memory: 15 835 660 288 bytes (approximately 15.84 GB).\n",
        "   - Max Threads per Block: 1024\n",
        "   - Max Block Dimensions: (1024, 1024, 64)\n",
        "   - Max Grid Dimensions: (2 147 483 647, 65 535, 65 535)\n",
        "\n",
        "2) What is the theoretical limit of blocks you can have?\n",
        "- The maximum number of blocks is limited by the max grid dimensions. For the Tesla T4, the limit is 2 147 483 647 blocks in the x dimension and 65 535 blocks in both the y and z dimensions. However, the total number of blocks that can be launched is practically limited by the number of threads that need to run and the size of data being processed, not just the maximum grid size.\n",
        "\n",
        "3) What is the theoretical limit of threads you can have?\n",
        "- The maximum number of threads within a block is 1 024, and given the maximum grid dimensions, we can calculate the theoretical limit of threads in a grid as follows:\n",
        "  - For the x dimension: 2 147 483 647 blocks * 1 024 threads/block\n",
        "  - For the y and z dimensions, the number of threads would be limited by the max threads per block since the maximum dimensions for y and z are the same as the max threads per block limit.\n",
        "\n",
        "4) Present a graph showing the time of the GPU, when you run the vector addition\n",
        "(vecAdd) with the strides of 1, 2, 8, 16, 32.\n",
        "- See the Excel File.\n"
      ],
      "metadata": {
        "id": "V0_sMVbwxyrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// CUDA kernel to add elements of two arrays\n",
        "__global__ void vecAdd(int stride, int N, int* a, int* b, int* c) {\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (i < N) {\n",
        "    c[i] = a[(i*stride)%N] + b[(i*stride)%N];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "  cudaDeviceReset();\n",
        "  int *a, *b, *c; // Host copies of a, b, c\n",
        "  int *d_a, *d_b, *d_c; // Device copies of a, b, c\n",
        "  int n = 40960000;\n",
        "  int stride = 32; //Change this value HERE\n",
        "  int size = n * sizeof(int); // Size of the arrays in bytes\n",
        "\n",
        "  // Allocate space for host copies and setup values\n",
        "  a = (int *)malloc(size);\n",
        "  b = (int *)malloc(size);\n",
        "  c = (int *)malloc(size);\n",
        "\n",
        "  // Initialize arrays with data\n",
        "  for (int i = 0; i < n; i++) {\n",
        "    a[i] = i;\n",
        "    b[i] = i;\n",
        "  }\n",
        "\n",
        "  // Allocate space for device copies\n",
        "  cudaMalloc((void **)&d_a, size);\n",
        "  cudaMalloc((void **)&d_b, size);\n",
        "  cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "  // Copy inputs to device\n",
        "  cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Setup timing events\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // Record event on the default stream\n",
        "  cudaEventRecord(start, 0);\n",
        "\n",
        "  // Launch vecAdd() kernel on GPU with n/1024 blocks of 1024 threads each\n",
        "  vecAdd<<<(n + 1023)/1024, 1024>>>(stride, n, d_a, d_b, d_c);\n",
        "  cudaEventRecord(stop, 0);\n",
        "  cudaEventSynchronize(stop);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Copy result back to host\n",
        "  cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Record event on the default stream\n",
        "\n",
        "  // Calculate elapsed time\n",
        "  float milliseconds = 0;\n",
        "  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "  printf(\"Time for vector addition: %f ms\\n\", milliseconds);\n",
        "\n",
        "  // Cleanup\n",
        "  free(a);\n",
        "  free(b);\n",
        "  free(c);\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bguU6zKc3das",
        "outputId": "1f7833c5-e2ab-42dd-948d-97966bfa624f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for vector addition: 22.959040 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following code, one sequential sum, followed by only a parallel one (which is currently in its worst form) and your GPU characteristics\n",
        "1. What is the theoretical speed-up limit considering infinite cores?\n",
        "  - Amdahl's Law states that the speed-up of a program using multiple processors in parallel computing is limited by the sequential fraction of the program. For portions of the program that cannot be parallelized, the speed-up will be limited no matter how many processors are used.\n",
        "\n",
        "  - Since the question posits \"infinite cores,\" the non-parallelizable portion becomes the bottleneck. However, because the sum can be entirely parallelized, the speed-up with infinite cores would be infinitely large in theory.\n",
        "\n",
        "2. What is the theoretical speed-up limit considering your machine?\n",
        "  - The Tesla T4 GPU has a compute capability of 7.5, allowing a maximum of 1024 threads per block. Given that the serial version of the sum is completely parallelizable, the maximum theoretical speed-up would be the number of threads that can be actively working on the problem simultaneously. However, in reality, the speed-up is limited by other factors, such as memory bandwidth, latency, and the overhead of launching CUDA kernels.\n",
        "\n",
        "  - If we consider the maximum threads per block (1024) and assume that each thread can handle one element addition, the theoretical upper bound would be 1024 times faster than the serial code. However, the actual speed-up will likely be less due to overheads and inefficiencies.\n",
        "3. What is the speed-up achieved from having the efficient parallel part?\n",
        "- $Speed-up = \\frac{Serial Exec Time}{Parallel Exec Time}$\n",
        "- For the inefficient part, the speed-up is approximately 65.34 and for the efficient part, the speed-up is approximately 67.17.\n"
      ],
      "metadata": {
        "id": "7TEZPl5B5gBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <iostream>\n",
        "#include <numeric>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "//CUDA kernel for parallel reduction\n",
        "__global__ void parallelSum(int* d_data, int* d_result, int n) {\n",
        "    extern __shared__ int shared_data[];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load input into __shared__ memory\n",
        "    shared_data[tid] = (i < n) ? d_data[i] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Parallel reduction\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            shared_data[tid] += shared_data[tid + s];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(d_result, shared_data[0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void inefficientParallelSum(int* d_data, int* d_result, int n) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = gridDim.x * blockDim.x; // Total number of threads in the grid\n",
        "\n",
        "    // Initialize local sum\n",
        "    int local_sum = 0;\n",
        "\n",
        "    // Iterate over the array in steps of 'stride'\n",
        "    for (int i = index; i < n; i += stride) {\n",
        "        local_sum += d_data[i];\n",
        "    }\n",
        "\n",
        "    // Use atomic add to accumulate the results\n",
        "    atomicAdd(d_result, local_sum);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 1 << 20; // Example size (approximately 1 million elements)\n",
        "    std::vector<int> h_data(n, 1); // Initialized with 1 for simplicity\n",
        "\n",
        "    // Start timing the serial part\n",
        "    auto start_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Serial part: Summing the array\n",
        "    int serial_sum = std::accumulate(h_data.begin(), h_data.end(), 0);\n",
        "\n",
        "    // Stop timing the serial part\n",
        "    auto end_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Calculate and print the duration of the serial part\n",
        "    std::chrono::duration<double> duration_serial = end_serial - start_serial;\n",
        "    std::cout << \"Serial sum: \" << serial_sum << std::endl;\n",
        "    std::cout << \"Serial duration: \" << duration_serial.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Parallel part\n",
        "    int* d_data;\n",
        "    int* d_result;\n",
        "    int parallel_sum = 0;\n",
        "    float efficient_milliseconds = 0;  // Timing variable for the efficient kernel\n",
        "    float inefficient_milliseconds = 0;  // Timing variable for the inefficient kernel\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc(&d_data, n * sizeof(int));\n",
        "    cudaMalloc(&d_result, sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_data, h_data.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_result, 0, sizeof(int)); // Initialize result on device to 0\n",
        "\n",
        "    // Determine block and grid sizes for the efficient parallel sum\n",
        "    int blockSize = 1024;  // Use the maximum threads per block based on your device capability\n",
        "    int gridSize = (n + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Launch the efficient kernel and time it\n",
        "    cudaEventRecord(start);\n",
        "    parallelSum<<<gridSize, blockSize, blockSize * sizeof(int)>>>(d_data, d_result, n);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaEventElapsedTime(&efficient_milliseconds, start, stop);\n",
        "\n",
        "    cudaMemcpy(&parallel_sum, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    std::cout << \"Good Parallel sum: \" << parallel_sum << std::endl;\n",
        "    std::cout << \"Good Parallel duration: \" << efficient_milliseconds << \" ms\" << std::endl;\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    // Reallocate device memory for the inefficient parallel sum\n",
        "    cudaMalloc(&d_data, n * sizeof(int));\n",
        "    cudaMalloc(&d_result, sizeof(int));\n",
        "    cudaMemcpy(d_data, h_data.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_result, 0, sizeof(int)); // Reset result on device to 0\n",
        "\n",
        "    // Launch the inefficient kernel with a better configuration and time it\n",
        "    gridSize = 1;  // Use a single grid\n",
        "    blockSize = 256; // Reasonable guess for number of threads (modify based on your device's capability)\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    inefficientParallelSum<<<gridSize, blockSize>>>(d_data, d_result, n);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaEventElapsedTime(&inefficient_milliseconds, start, stop);\n",
        "\n",
        "    cudaMemcpy(&parallel_sum, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Inefficient Parallel sum: \" << parallel_sum << std::endl;\n",
        "    std::cout << \"Inefficient Parallel duration: \" << inefficient_milliseconds << \" ms\" << std::endl;\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd56dkgF-CRd",
        "outputId": "a188037f-99ad-4567-a97a-474e5b622bd5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial sum: 1048576\n",
            "Serial duration: 0.0184296 seconds\n",
            "Good Parallel sum: 1048576\n",
            "Good Parallel duration: 0.27472 ms\n",
            "Inefficient Parallel sum: 1048576\n",
            "Inefficient Parallel duration: 0.29584 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <numeric>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "// CUDA kernel for parallel reduction\n",
        "__global__ void parallelSum(int* d_data, int* d_result, int n) {\n",
        "    extern __shared__ int shared_data[];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load input into __shared__ memory\n",
        "    shared_data[tid] = (i < n) ? d_data[i] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Parallel reduction\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            shared_data[tid] += shared_data[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(d_result, shared_data[0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 1 << 20; // Example size (approximately 1 million elements)\n",
        "    std::vector<int> h_data(n, 1); // Initialized with 1 for simplicity\n",
        "\n",
        "    // Start timing the serial part\n",
        "    auto start_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Serial part: Summing the array\n",
        "    int serial_sum = std::accumulate(h_data.begin(), h_data.end(), 0);\n",
        "\n",
        "    // Stop timing the serial part\n",
        "    auto end_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Calculate and print the duration of the serial part\n",
        "    std::chrono::duration<double> duration_serial = end_serial - start_serial;\n",
        "    std::cout << \"Serial sum: \" << serial_sum << std::endl;\n",
        "    std::cout << \"Serial duration: \" << duration_serial.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Parallel part\n",
        "    int* d_data;\n",
        "    int* d_result;\n",
        "    int parallel_sum = 0;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Reallocate device memory for the parallel sum\n",
        "    cudaMalloc(&d_data, n * sizeof(int));\n",
        "    cudaMalloc(&d_result, sizeof(int));\n",
        "    cudaMemcpy(d_data, h_data.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_result, 0, sizeof(int)); // Reset result on device to 0\n",
        "\n",
        "    // Start timing the parallel part\n",
        "    auto start_parallel = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Determine block and grid sizes\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (n + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Launch kernel\n",
        "    parallelSum<<<gridSize, blockSize, blockSize * sizeof(int)>>>(d_data, d_result, n);\n",
        "\n",
        "    // Stop timing the parallel part\n",
        "    auto end_parallel = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Copy back the result and free device memory\n",
        "    cudaMemcpy(&parallel_sum, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    // Calculate and print the duration of the parallel part\n",
        "    std::chrono::duration<double> duration_parallel = end_parallel - start_parallel;\n",
        "\n",
        "    std::cout << \"Good Parallel sum: \" << parallel_sum << std::endl;\n",
        "    std::cout << \"Good Parallel duration: \" << duration_parallel.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQzMQ_B4_tQP",
        "outputId": "d8e4cda9-cbe3-441f-c74e-f393fb9b3a56"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial sum: 1048576\n",
            "Serial duration: 0.0149396 seconds\n",
            "Good Parallel sum: 1048576\n",
            "Good Parallel duration: 0.000222403 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <numeric>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "// CUDA kernel for parallel reduction\n",
        "__global__ void parallelSum(int* d_data, int* d_result, int n) {\n",
        "    extern __shared__ int shared_data[];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load input into __shared__ memory\n",
        "    shared_data[tid] = (i < n) ? d_data[i] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Parallel reduction\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            shared_data[tid] += shared_data[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(d_result, shared_data[0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void inefficientParallelSum(int* d_data, int* d_result, int n) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = gridDim.x * blockDim.x; // Total number of threads in the grid\n",
        "\n",
        "    // Initialize local sum\n",
        "    int local_sum = 0;\n",
        "\n",
        "    // Iterate over the array in steps of 'stride'\n",
        "    for (int i = index; i < n; i += stride) {\n",
        "        local_sum += d_data[i];\n",
        "    }\n",
        "\n",
        "    // Use atomic add to accumulate the results\n",
        "    atomicAdd(d_result, local_sum);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 1 << 20; // Example size (approximately 1 million elements)\n",
        "    std::vector<int> h_data(n, 1); // Initialized with 1 for simplicity\n",
        "\n",
        "    // Start timing the serial part\n",
        "    auto start_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Serial part: Summing the array\n",
        "    int serial_sum = std::accumulate(h_data.begin(), h_data.end(), 0);\n",
        "\n",
        "    // Stop timing the serial part\n",
        "    auto end_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Calculate and print the duration of the serial part\n",
        "    std::chrono::duration<double> duration_serial = end_serial - start_serial;\n",
        "    std::cout << \"Serial sum: \" << serial_sum << std::endl;\n",
        "    std::cout << \"Serial duration: \" << duration_serial.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Parallel part\n",
        "    int* d_data;\n",
        "    int* d_result;\n",
        "    int parallel_sum = 0;\n",
        "\n",
        "    // Reallocate device memory for the inefficient parallel sum\n",
        "    cudaMalloc(&d_data, n * sizeof(int));\n",
        "    cudaMalloc(&d_result, sizeof(int));\n",
        "    cudaMemcpy(d_data, h_data.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_result, 0, sizeof(int)); // Reset result on device to 0\n",
        "    // Start timing the parallel part\n",
        "    auto start_parallel = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "\n",
        "    // Launch kernel\n",
        "    inefficientParallelSum<<<1, 2, 2 * sizeof(int)>>>(d_data, d_result, n);\n",
        "\n",
        "    // Stop timing the parallel part\n",
        "    auto end_parallel = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Copy back the result and free device memory\n",
        "    cudaMemcpy(&parallel_sum, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    // Calculate and print the duration of the parallel part\n",
        "    std::chrono::duration<double> duration_parallel = end_parallel - start_parallel;\n",
        "\n",
        "    std::cout << \"Inefficient Parallel sum: \" << parallel_sum << std::endl;\n",
        "    std::cout << \"Inefficient Parallel duration: \" << duration_parallel.count() << \"seconds\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jG8yXqE_8Ez",
        "outputId": "2b143c73-d2f1-48a7-e09e-fab8f62f8237"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial sum: 1048576\n",
            "Serial duration: 0.0168273 seconds\n",
            "Inefficient Parallel sum: 1048576\n",
            "Inefficient Parallel duration: 0.000257504seconds\n",
            "\n"
          ]
        }
      ]
    }
  ]
}