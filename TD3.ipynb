{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture_1.png](picture_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the GPU architecture in the picture above, how many Stream \n",
    "Multiprocessors (SMs) does it have? And how many cores does it have? \n",
    "\n",
    "There is 16 SMs and 512 cores.\n",
    "\n",
    "Considering that 1,2 and 3 are the representations of 3 different BLOCKS that someone is planning to execute simultaneously, which one(s) is(are) valid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture_2.png](picture_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pycuda.autoinit \n",
    "import pycuda.driver as cuda \n",
    "from pycuda.compiler import SourceModule \n",
    "import time \n",
    "\n",
    "# Set GPU architecture for gtx 1060 \n",
    "cuda.Context.get_device().make_context()\n",
    " \n",
    "# Define the size of the vectors \n",
    "N = 360000000\n",
    "\n",
    "# Create two random vectors and a destination vector \n",
    "a = np.random.randn(N).astype(np.float32) \n",
    "b = np.random.randn(N).astype(np.float32) \n",
    "c = np.zeros_like(a) \n",
    " \n",
    "# Allocate memory on the device and copy data from the host \n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a) \n",
    "cuda.memcpy_htod(b_gpu, b) \n",
    " \n",
    "# Compile the kernel code \n",
    "mod = SourceModule(\"\"\" \n",
    "__global__ void vector_add_coarse_grained(float *a, float *b, float *c, int n) { \n",
    "    int index = (threadIdx.x + blockIdx.x * blockDim.x) * 2;  // Each thread will handle 2 elements \n",
    "    if (index < n) { \n",
    "        c[index] = a[index] + b[index]; \n",
    "        if (index + 1 < n) { \n",
    "            c[index + 1] = a[index + 1] + b[index + 1]; \n",
    "        } \n",
    "    } \n",
    "} \n",
    " \n",
    "__global__ void vector_add_fine_grained(float *a, float *b, float *c, int n) { \n",
    "    int index = threadIdx.x + blockIdx.x * blockDim.x; \n",
    "    if (index < n) { \n",
    "        c[index] = a[index] + b[index]; \n",
    "    } \n",
    "} \n",
    "\"\"\") \n",
    " \n",
    "# Get the kernel function from the compiled module \n",
    "vector_add_fine = mod.get_function(\"vector_add_fine_grained\") \n",
    "vector_add_coarse = mod.get_function(\"vector_add_coarse_grained\") \n",
    " \n",
    "# Launch the kernel \n",
    "threads_per_block = 1024 \n",
    "blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "# Choose one of these based on the granularity you want \n",
    "start_time = time.time() \n",
    "vector_add_fine(a_gpu, b_gpu, c_gpu, np.int32(N), block=(threads_per_block, 1, 1), grid=(blocks_per_grid, 1)) \n",
    "cuda.Context.synchronize()  # Wait for the kernel to finish \n",
    "fine_time = time.time() - start_time \n",
    " \n",
    "start_time = time.time() \n",
    "vector_add_coarse(a_gpu, b_gpu, c_gpu, np.int32(N), block=(threads_per_block, 1, 1), grid=(blocks_per_grid, 1)) \n",
    "cuda.Context.synchronize()  # Wait for the kernel to finish \n",
    "coarse_time = time.time() - start_time \n",
    " \n",
    "# Copy the result back to the host \n",
    "cuda.memcpy_dtoh(c, c_gpu) \n",
    " \n",
    "# Check the result \n",
    "if not np.allclose(c, a + b): \n",
    "    print(\"Discrepancy found in the results!\") \n",
    "else: \n",
    "    print(\"Results are consistent with NumPy's computation.\") \n",
    " \n",
    "    # Print the timing results \n",
    "print(f\"Fine-grained kernel execution time: {fine_time:.6f} seconds\") \n",
    "print(f\"Coarse-grained kernel execution time: {coarse_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are consistent with NumPy's computation.\\\n",
    "Fine-grained kernel execution time: 0.017520 seconds\\\n",
    "Coarse-grained kernel execution time: 0.019040 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "#include <stdio.h>\n",
    "__global__ void vecAdd() {\n",
    "    int i = blockIdx.x;\n",
    "    printf(\"Hi from Block %d\\\\n\", i);\n",
    "}\n",
    "\"\"\")\n",
    "vecAdd = mod.get_function(\"vecAdd\")\n",
    "vecAdd(block=(12, 1, 1))\n",
    "cuda.Context.synchronize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi from Block 2\\\n",
    "Hi from Block 8\\\n",
    "Hi from Block 5\\\n",
    "Hi from Block 0\\\n",
    "Hi from Block 11\\\n",
    "Hi from Block 6\\\n",
    "Hi from Block 3\\\n",
    "Hi from Block 9\\\n",
    "Hi from Block 1\\\n",
    "Hi from Block 7\\\n",
    "Hi from Block 4\\\n",
    "Hi from Block 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Blocks were executed in a random order, this is due to the fact that the scheduler is responsible for the execution of the blocks and it is not possible to predict the order in which the blocks will be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a CUDA kernel to perform element-wise addition of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "__global__ void vector_add(float *a, float *b, float *c, int n) {\n",
    "    int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (index < n) {\n",
    "        c[index] = a[index] + b[index];\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "N = 1000000\n",
    "a = np.random.randn(N).astype(np.float32)\n",
    "b = np.random.randn(N).astype(np.float32)\n",
    "c = np.zeros_like(a)\n",
    "\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "vector_add = mod.get_function(\"vector_add\")\n",
    "vector_add(a_gpu, b_gpu, c_gpu, np.int32(N), block=(1024, 1, 1), grid=(N // 1024 + 1, 1))\n",
    "cuda.Context.synchronize()\n",
    "\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "print(c)\n",
    "print(a + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 1.5640146  -2.260321    0.89957166 ...  0.60166585 -0.6229739\n",
    "  0.5706136 ]\\\n",
    "[ 1.5640146  -2.260321    0.89957166 ...  0.60166585 -0.6229739\n",
    "  0.5706136 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the vector addition kernel to use shared memory for one of the input vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "__global__ void vector_add_shared(float *a, float *b, float *c, int n) {\n",
    "    extern __shared__ float shared_a[];  // Declare shared memory for vector a\n",
    "    int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    // Load data from global memory to shared memory\n",
    "    if (threadIdx.x < n) {\n",
    "        shared_a[threadIdx.x] = a[index];\n",
    "    }\n",
    "    __syncthreads();  // Synchronize threads to ensure all data is loaded\n",
    "    \n",
    "    // Perform element-wise addition using shared memory\n",
    "    if (index < n) {\n",
    "        c[index] = shared_a[threadIdx.x] + b[index];\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "block_size = 1024\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "\n",
    "vector_add_shared = mod.get_function(\"vector_add_shared\")\n",
    "vector_add_shared(a_gpu, b_gpu, c_gpu, np.int32(N), block=(block_size, 1, 1), grid=(grid_size, 1), shared=4 * block_size)\n",
    "cuda.Context.synchronize()\n",
    "\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "print(c)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 1.5640146  -2.260321    0.89957166 ...  0.60166585 -0.6229739\n",
    "  0.5706136 ]\\\n",
    "[ 1.5640146  -2.260321    0.89957166 ...  0.60166585 -0.6229739\n",
    "  0.5706136 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a tiled version of matrix multiplication using shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# Define the matrix sizes\n",
    "M = 1024\n",
    "N = 1024\n",
    "K = 1024\n",
    "\n",
    "# Create random matrices A and B\n",
    "A = np.random.randn(M, K).astype(np.float32)\n",
    "B = np.random.randn(K, N).astype(np.float32)\n",
    "\n",
    "# Allocate memory for the result matrix C\n",
    "C = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "# Define the tile size\n",
    "tile_size = 32\n",
    "\n",
    "# Define the number of tiles\n",
    "num_tiles = M // tile_size\n",
    "\n",
    "# Define the shared memory size\n",
    "shared_mem_size = tile_size * tile_size * 4\n",
    "\n",
    "# Define the kernel code\n",
    "kernel_code = \"\"\"\n",
    "__global__ void matrix_multiply_tiled(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    // Define shared memory for the tile\n",
    "    __shared__ float tile_A[%(tile_size)s][%(tile_size)s];\n",
    "    __shared__ float tile_B[%(tile_size)s][%(tile_size)s];\n",
    "\n",
    "    // Calculate the global row and column indices\n",
    "    int row = blockIdx.y * %(tile_size)s + threadIdx.y;\n",
    "    int col = blockIdx.x * %(tile_size)s + threadIdx.x;\n",
    "\n",
    "    // Initialize the result for this thread\n",
    "    float result = 0.0;\n",
    "\n",
    "    // Loop over the tiles of the input matrices\n",
    "    for (int t = 0; t < %(num_tiles)s; t++) {\n",
    "        // Load the tiles into shared memory\n",
    "        tile_A[threadIdx.y][threadIdx.x] = A[row * K + t * %(tile_size)s + threadIdx.x];\n",
    "        tile_B[threadIdx.y][threadIdx.x] = B[(t * %(tile_size)s + threadIdx.y) * N + col];\n",
    "\n",
    "        // Synchronize to make sure the tiles are loaded\n",
    "        __syncthreads();\n",
    "\n",
    "        // Perform the matrix multiplication for the current tile\n",
    "        for (int k = 0; k < %(tile_size)s; k++) {\n",
    "            result += tile_A[threadIdx.y][k] * tile_B[k][threadIdx.x];\n",
    "        }\n",
    "\n",
    "        // Synchronize to make sure the result is updated\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result to the output matrix\n",
    "    C[row * N + col] = result;\n",
    "}\n",
    "\"\"\" % {'tile_size': tile_size, 'num_tiles': num_tiles}\n",
    "\n",
    "# Compile the kernel code\n",
    "mod = pycuda.compiler.SourceModule(kernel_code)\n",
    "\n",
    "# Get the kernel function from the compiled module\n",
    "matrix_multiply_tiled = mod.get_function(\"matrix_multiply_tiled\")\n",
    "\n",
    "# Allocate memory on the GPU\n",
    "A_gpu = cuda.mem_alloc(A.nbytes)\n",
    "B_gpu = cuda.mem_alloc(B.nbytes)\n",
    "C_gpu = cuda.mem_alloc(C.nbytes)\n",
    "\n",
    "# Copy the input matrices to the GPU\n",
    "cuda.memcpy_htod(A_gpu, A)\n",
    "cuda.memcpy_htod(B_gpu, B)\n",
    "\n",
    "# Define the grid and block dimensions\n",
    "block_dim = (tile_size, tile_size, 1)\n",
    "grid_dim = (N // tile_size, M // tile_size, 1)\n",
    "\n",
    "# Launch the kernel\n",
    "matrix_multiply_tiled(A_gpu, B_gpu, C_gpu, np.int32(M), np.int32(N), np.int32(K), block=block_dim, grid=grid_dim)\n",
    "\n",
    "# Copy the result matrix back to the CPU\n",
    "cuda.memcpy_dtoh(C, C_gpu)\n",
    "\n",
    "# Print the result matrix\n",
    "print(np.matmul(A, B))\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ 36.501495  -25.381676   32.539627  ... -11.418268   -9.357998\\\n",
    "  -17.958183 ]\\\n",
    " [ 10.755127   -9.259977  -54.297485  ...  32.701073  -10.968725\\\n",
    "   -9.568348 ]\\\n",
    " [ 39.094963   -7.4902244  33.729324  ...  36.56028    91.71763\\\n",
    "   19.633055 ]\\\n",
    " ...\\\n",
    " [-18.937649   -8.362484   18.537598  ...  18.997225   -3.654273\\\n",
    "  -23.108137 ]\\\n",
    " [-28.292513  -45.573208   30.893597  ...  -5.900941   14.504358\\\n",
    "   32.278687 ]\\\n",
    " [-34.52884    15.688816    8.461049  ...  34.496838  -29.627739\\\n",
    "  -23.486835 ]]\\\n",
    "[[ 36.501495  -25.381676   32.539635  ... -11.418284   -9.357988\\\n",
    "  -17.958187 ]\\\n",
    " [ 10.755133   -9.259976  -54.297478  ...  32.701046  -10.9687195\\\n",
    "   -9.568327 ]\\\n",
    " [ 39.09495    -7.4902477  33.7293    ...  36.56028    91.71768\\\n",
    "   19.633053 ]\\\n",
    " ...\\\n",
    " [-18.937649   -8.36248    18.537575  ...  18.997227   -3.6542697\\\n",
    "  -23.108149 ]\\\n",
    " [-28.292515  -45.573162   30.893604  ...  -5.900946   14.504358\\\n",
    "   32.278698 ]\\\n",
    " [-34.528816   15.688814    8.461073  ...  34.496845  -29.627726 \\\n",
    "  -23.486832 ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an average on a 2D grid using shared memory to cache neighbouring elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a parallel reduction in shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "# Define the input data\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.int32)\n",
    "\n",
    "# Allocate memory on the GPU\n",
    "data_gpu = cuda.mem_alloc(data.nbytes)\n",
    "\n",
    "# Copy the input data to the GPU\n",
    "cuda.memcpy_htod(data_gpu, data)\n",
    "\n",
    "# Define the kernel code\n",
    "kernel_code = \"\"\"\n",
    "__global__ void parallel_reduction(int* data, int* result) {\n",
    "    // Define shared memory for the block\n",
    "    __shared__ int shared_data[%(block_size)s];\n",
    "\n",
    "    // Calculate the global index\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Load the data into shared memory\n",
    "    shared_data[threadIdx.x] = data[index];\n",
    "\n",
    "    // Synchronize to make sure the data is loaded\n",
    "    __syncthreads();\n",
    "\n",
    "    // Perform the reduction\n",
    "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
    "        if (threadIdx.x < stride) {\n",
    "            shared_data[threadIdx.x] += shared_data[threadIdx.x + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Store the result in global memory\n",
    "    if (threadIdx.x == 0) {\n",
    "        result[blockIdx.x] = shared_data[0];\n",
    "    }\n",
    "}\n",
    "\"\"\" % {'block_size': len(data)}\n",
    "\n",
    "# Compile the kernel code\n",
    "mod = SourceModule(kernel_code)\n",
    "\n",
    "# Get the kernel function from the compiled module\n",
    "parallel_reduction = mod.get_function(\"parallel_reduction\")\n",
    "\n",
    "# Define the block and grid dimensions\n",
    "block_dim = (len(data), 1, 1)\n",
    "grid_dim = (1, 1, 1)\n",
    "\n",
    "# Allocate memory for the result\n",
    "result = np.zeros(1, dtype=np.int32)\n",
    "result_gpu = cuda.mem_alloc(result.nbytes)\n",
    "\n",
    "# Launch the kernel\n",
    "parallel_reduction(data_gpu, result_gpu, block=block_dim, grid=grid_dim)\n",
    "\n",
    "# Copy the result back to the CPU\n",
    "cuda.memcpy_dtoh(result, result_gpu)\n",
    "\n",
    "# Print the result\n",
    "print(\"Sum:\", result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum: 40\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
